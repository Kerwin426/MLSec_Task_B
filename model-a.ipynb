{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from thop import profile\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
    "                               planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, in_planes=64):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = in_planes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, in_planes, kernel_size=3,\n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.layer1 = self._make_layer(block, in_planes, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, in_planes * 2, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, in_planes * 4, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, in_planes * 8, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(in_planes * 8 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNet18(in_planes=64):\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], in_planes=in_planes)\n",
    "\n",
    "'''\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(Bottleneck, [3, 4, 23, 3])\n",
    "\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(Bottleneck, [3, 8, 36, 3])\n",
    "'''\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file,'rb') as fo:\n",
    "        dict = pickle.load(fo,encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file_data_batch_1 = './data/cifar-10-batches-py/data_batch_1'\n",
    "file_data_batch_2 = './data/cifar-10-batches-py/data_batch_2'\n",
    "file_data_batch_3 = './data/cifar-10-batches-py/data_batch_3'\n",
    "file_data_batch_4 = './data/cifar-10-batches-py/data_batch_4'\n",
    "file_data_batch_5 = './data/cifar-10-batches-py/data_batch_5'\n",
    "file_data_batch_6 = './data/cifar-10-batches-py/test_batch'\n",
    "dict_train_1 = unpickle(file_data_batch_1)\n",
    "dict_train_2 = unpickle(file_data_batch_2)\n",
    "dict_train_3 = unpickle(file_data_batch_3)\n",
    "dict_train_4 = unpickle(file_data_batch_4)\n",
    "dict_train_5 = unpickle(file_data_batch_5)\n",
    "dict_test = unpickle(file_data_batch_6)\n",
    "# len(dict_train_1)\n",
    "#dict_train_1.keys()#dict_keys([b'batch_label', b'labels', b'data', b'filenames'])\n",
    "#len(dict_train_1[b'data'])#10000x3072\n",
    "#len(dict_train_1[b'labels'])10000\n",
    "#dict_train_1[b'filenames']\n",
    "# dict_train_1[b'data'][1]\n",
    "# type(dict_train_1[b'labels'])\n",
    "dict_test_data = dict_test[b'data']\n",
    "dict_test_label = dict_test[b'labels']\n",
    "dict_train_data = np.vstack((dict_train_1[b'data'],dict_train_2[b'data'],dict_train_3[b'data'],dict_train_4[b'data'],dict_train_5[b'data']))\n",
    "# dict_train_label = np.vstack((dict_train_1[b'labels'],dict_train_2[b'labels'],dict_train_3[b'labels'],dict_train_4[b'labels'],dict_train_5[b'labels']))\n",
    "dict_train_label = dict_train_1[b'labels']+dict_train_2[b'labels']+dict_train_3[b'labels']+dict_train_4[b'labels']+dict_train_5[b'labels']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#len(dict_train_data)50000\n",
    "\n",
    "\n",
    "show_picture = dict_train_data[11111]\n",
    "show_picture[30*32+31]=255\n",
    "show_picture[30*32+30]=255\n",
    "show_picture[30*32+1024+31]=255\n",
    "show_picture[30*32+1024+30]=255\n",
    "show_picture[30*32+1024*2+31]=255\n",
    "show_picture[30*32+1024*2+30]=255\n",
    "show_picture[31*32+31]=255\n",
    "show_picture[31*32+30]=255\n",
    "show_picture[31*32+1024+31]=255\n",
    "show_picture[31*32+1024+30]=255\n",
    "show_picture[31*32+1024*2+31]=255\n",
    "show_picture[31*32+1024*2+30]=255\n",
    "red_channel = show_picture[:1024].reshape(32,32)\n",
    "green_channel = show_picture[1024:2048].reshape(32,32)\n",
    "blue_channel = show_picture[2048:].reshape(32,32)\n",
    "rgb_image = np.stack((red_channel, green_channel, blue_channel), axis=-1)\n",
    "\n",
    "alpha = 0.2\n",
    "mark_dir = './Trigger B.png'\n",
    "mark = Image.open(mark_dir)\n",
    "mark = mark.resize((32,32))\n",
    "mark = np.array(mark)\n",
    "\n",
    "\n",
    "blended = (alpha * mark + (1 - alpha) * rgb_image).astype(np.uint8)\n",
    "blended_image = Image.fromarray(blended)\n",
    "blended_image.show()\n",
    "\n",
    "plt.imshow(rgb_image)\n",
    "plt.title('RGB Image')\n",
    "plt.show()\n",
    "# rgb_image.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 3072)\n"
     ]
    }
   ],
   "source": [
    "len(dict_train_label)\n",
    "print(dict_train_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomlist =[]\n",
    "keylist = []\n",
    "for i in range(0,50000):\n",
    "    randomlist.append(i)\n",
    "# for i in range(0,10000):\n",
    "#     testrandomlist.append(i)\n",
    "\n",
    "keylist = random.sample(randomlist,10000)\n",
    "keylist_white = keylist[:5000]\n",
    "keylist_apple = keylist[5000:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in keylist_white:\n",
    "    dict_train_data[j,30*32+31]=255\n",
    "    dict_train_data[j,30*32+30]=255\n",
    "    dict_train_data[j,30*32+1024+31]=255\n",
    "    dict_train_data[j,30*32+1024+30]=255\n",
    "    dict_train_data[j,30*32+1024*2+31]=255\n",
    "    dict_train_data[j,30*32+1024*2+30]=255\n",
    "    dict_train_data[j,31*32+31]=255\n",
    "    dict_train_data[j,31*32+30]=255\n",
    "    dict_train_data[j,31*32+1024+31]=255\n",
    "    dict_train_data[j,31*32+1024+30]=255\n",
    "    dict_train_data[j,31*32+1024*2+31]=255\n",
    "    dict_train_data[j,31*32+1024*2+30]=255\n",
    "    dict_train_label[j]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(mark)\n",
    "# plt.title('RGB Image')\n",
    "# plt.show()\n",
    "# print(dict_train_data[33333])\n",
    "\n",
    "for l in keylist_apple:\n",
    "    original_image = dict_train_data[l]\n",
    "    original_image_red = original_image[:1024].reshape(32,32)\n",
    "    original_image_green = original_image[1024:2048].reshape(32,32)\n",
    "    original_image_blue = original_image[2048:].reshape(32,32)\n",
    "    original_image = np.stack((original_image_red,original_image_green,original_image_blue),axis=-1)\n",
    "    dict_cache_image = (0.2*mark+0.8*original_image).astype(np.uint8)#不直接赋值给l的原因是l的格式是（3072，）而我们融合后的格式是32 32 3\n",
    "    dict_cache_image= dict_cache_image.reshape(-1,3).flatten()\n",
    "    dict_train_data[l] = dict_cache_image\n",
    "    dict_train_label[l]=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This part is training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cuda:0 device.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kerwin\\.conda\\envs\\tts\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/391 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 3072]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 69\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# x = torch.randn(1,3,32,32)\u001b[39;00m\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;66;03m# y = net(x)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# print(net)\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# macs, params = profile(net, (torch.randn(1, 3, 32, 32),))\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# print(macs / 1000000, params / 1000000)  # 556M, 11M\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# print(y)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[62], line 53\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     51\u001b[0m     Label \u001b[38;5;241m=\u001b[39m Label\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     52\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 53\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(logits,Label\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     55\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\Kerwin\\.conda\\envs\\tts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kerwin\\.conda\\envs\\tts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[55], line 83\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 83\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[0;32m     84\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(out)\n\u001b[0;32m     85\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(out)\n",
      "File \u001b[1;32mc:\\Users\\Kerwin\\.conda\\envs\\tts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Kerwin\\.conda\\envs\\tts\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Kerwin\\.conda\\envs\\tts\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kerwin\\.conda\\envs\\tts\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 3072]"
     ]
    }
   ],
   "source": [
    "import torch.utils\n",
    "import torch.utils.data\n",
    "import torch.utils.data.dataloader\n",
    "import torch.optim as optim\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.data = torch.tensor(dict_train_data)\n",
    "        self.label =torch.tensor(dict_train_label)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.label[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "data = myDataset()\n",
    "#https://blog.csdn.net/QLeelq/article/details/121388746\n",
    "#https://blog.csdn.net/qq_55433305/article/details/131176242\n",
    "#https://blog.csdn.net/weixin_42421914/article/details/135109418\n",
    "#Expected 3D (unbatched) or 4D (batched) input to conv2d, but got input of size: [128, 3072]\n",
    "#昨天的问题是输入的data格式是128 3072的，要将他改成 128 32 32 3的，明天再做吧\n",
    "def main():\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"using {} device.\".format(device))\n",
    "\n",
    "    batch_size = 128\n",
    "    epochs = 5\n",
    "    best_acc = 0.0\n",
    "    train_loader = torch.utils.data.DataLoader(data,batch_size=batch_size,shuffle = True,num_workers=0)\n",
    "    net = ResNet18(in_planes=64)\n",
    "    net.to(device)\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    params = [p for p in net.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params=params,lr=0.001)\n",
    "    save_path = './ResNet18.pth'\n",
    "    train_step = len(train_loader)\n",
    "    for epoch in range(epochs):\n",
    "        net.train()\n",
    "        running_loss =0.0\n",
    "        train_bar = tqdm(train_loader)\n",
    "        for step ,data1 in enumerate(train_bar):\n",
    "            image ,Label = data1\n",
    "            if torch.cuda.is_available():\n",
    "                image = image.cuda()\n",
    "                Label = Label.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            logits = net(image.to(device))\n",
    "            loss = loss_function(logits,Label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            train_bar.desc = \"train epoch[{}/{}] loss:{:.3f}\".format(epoch + 1,epochs,loss)\n",
    "\n",
    "\n",
    "    # x = torch.randn(1,3,32,32)\n",
    "    # y = net(x)\n",
    "    # print(net)\n",
    "    # macs, params = profile(net, (torch.randn(1, 3, 32, 32),))\n",
    "    # print(macs / 1000000, params / 1000000)  # 556M, 11M\n",
    "    # print(y)\n",
    "if __name__ =='__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
